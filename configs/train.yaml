# @package _global_

# Order indicates overwriting
defaults:
  - _self_
  - trainer: default.yaml
  - model: dense.yaml
  - datamodule: pc_data.yaml
  - loggers: default.yaml
  - hydra: default.yaml
  - paths: default.yaml
  - callbacks: default.yaml
  - experiment: null

seed: 12345 # For reproducibility
project_name: mpm # Determines output directory path and wandb project
network_name: ${now:%Y-%m-%d}_${now:%H-%M-%S} # Used for both saving and wandb
train: True # Set False to skip model training
ckpt_path: null # Checkpoint path to resume training

# Extra tweaks available with the new pytorch version
precision: high # Should use medium if on ampere gpus
compile: null # Can set to default for faster compiles

# COMPLETELY replaces the above config with what is contained in ${paths.full_path}
# This is ideal for resuming a job, log to the same directory
# Will also resume the loggers and set the ckpt_path to the latest
full_resume: False
